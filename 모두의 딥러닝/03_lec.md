- hypothesis에서 costfunction의 최종목적은 값 들의 차를 줄이는 것
<br>
- Cost(W)를 그리면 어떻게 생겼을까? <br>
-> W=1, cost(W)=0 <br>
-> W=0, cost(w)=4.67 <br>
-> W=2, cost(w)=4.67 <br>
<br>
= 그래프는 이차방정식의 그래프 처럼 곡선으로 생겼다.<br>

## Gradient descent algorithm
1. 가장 최소의 값을 찾아주는 함수
2. 최소화 해야하는 여러 문제들에 적용이 가능함
3. cost(W, b)에서 가장 최소의 값을 구할 수 있는 W와 b의 값을 구해준다.
4. w1, w2등 여러 값이 있어도 적용이 가능하다.
<br>
- 어떻게 최소값을 찾는 걸까?
1. 아무곳에서나 시작이 가능
2. W를 조금씩 움직여 가면서 경사도를 따라 한단계씩 내려간다
3. 경사도를 계산해서 (W,b)의 값을 찾으며 내려간다
4. 반복한다
5. 끝 (미분 사용)

## Convex function
문제점 : 처음에 잡을 시작점이 달라 결과가 다르게 나오는 정형화된 값이 아닐 경우에는 어떻게 해야하는가? <br>
-> 이런 것을 가지고 cost(W)를 쓰면 값을 구할 수 있다
- 결국, 어떤 모양이든 cost(W)를 이용하면 값을 구할 수 있다
- 그래프가 이차방정식의 그래프의 모습을 하고 있는지만 신경써주면 된다.


